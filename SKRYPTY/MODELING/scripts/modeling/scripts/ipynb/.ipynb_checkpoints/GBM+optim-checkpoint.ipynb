{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------                      GBM                      ----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "from hyperopt import fmin, hp, tpe, Trials, STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:/Users/mmandziej001/Desktop/FCU/SCRIPTS/predictive_qc_lion_king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('model_training/data/PC/dataset_dummy_grouped_time_train_cat8.csv')\n",
    "test_data = pd.read_csv('model_training/data/PC/dataset_dummy_grouped_time_test_cat8.csv')\n",
    "\n",
    "# drop id of cases\n",
    "train_id = train_data.pop('Unique')\n",
    "test_id = test_data.pop('Unique')\n",
    "\n",
    "train_labels = train_data.pop('Label')\n",
    "test_labels = test_data.pop('Label')\n",
    "\n",
    "train_datestamp = train_data.pop('datestamp')\n",
    "test_datestamp = test_data.pop('datestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boruta_features = ['ScreenedParties', 'PartyType_Subsidiary', 'Critical_last_3_checklistsDR', 'ESR_Full_ESR_review', 'Minor_last_3_checklistsDR', 'Minor_last_3_checklistsPC', 'Major_last_3_checklistsPC', 'TLAssignedName_Yadav__N___Neha_', 'Cases_last_30_days_of_PC', 'Minor_last_10_checklistsDR', 'Critical_last_10_checklistsDR', 'Cases_last_5_days_of_PC', 'TLAssignedName_Makowska__M_M___Malgorzata_', 'TLAssignedName_Michalik__J___Justyna_', 'TeamExperience', 'HourNumeric', 'GroupCases', 'TLAssignedName_Jurojc__M___Mateusz_', 'ProcessingUnit_Gdansk', 'ProcessingUnit_MidCorp', 'FirstGroupCase', 'TLAssignedName_Jastrzebowska__S___Sonia_', 'Minor_last_10_checklistsPC', 'Major_last_10_checklistsPC', 'TLAssignedName_Marcos_Cantabrana__I___Ivan_', 'TLAssignedName_Wojciechowska__M___Magdalena_', 'ProjectExperience', 'Cases_last_30_days_of_DR', 'Cases_last_5_days_of_DR', 'TLAssignedName_Rybka__I_A___Izabela_Anna_', 'Major_last_10_checklistsDR', 'Major_last_3_checklistsDR', 'Major_last_5_checklistsDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_importance = pd.read_excel('model_training/data/ai_assistant_dumps/PC/\\\n",
    "8_FATCA_CRS/Boruta_variable_importance_pc.xlsx')\n",
    "boruta_importance = boruta_importance.sort_values(by=['normHits', 'meanImp'],\n",
    "                                                  ascending=True)\n",
    "boruta_features = list(\n",
    "    boruta_importance[boruta_importance['decision'] == 'Confirmed'].V7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[boruta_features] # used_columns, boruta_features\n",
    "train_data = train_data[boruta_features] # used_columns, boruta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TLAssignedName_Armannsson__G___Gabriela_', 'ScreenedParties', 'COFATCA/CRS_Major_last_10_checklists', 'PCFATCA/CRS_Major_last_5_checklists', 'CODocumentation_standard_Minor_last_5_checklists', 'COClient_Outreach_Major_last_5_checklists', 'PCDocumentation_standard_Major_last_10_checklists', 'DRClient_Outreach_Major_last_5_checklists', 'PCDocumentation_standard_Minor_last_10_checklists', 'PCDocumentation_standard_Major_last_5_checklists', 'PCDocumentation_standard_Minor_last_5_checklists', 'TeamExperience', 'PCFATCA/CRS_Critical_last_5_checklists', 'ProjectExperience', 'PCFATCA/CRS_Critical_last_10_checklists', 'DRClient_Outreach_Major_last_10_checklists', 'CRS_FALSE', 'CRS_TRUE', 'FATCA_FALSE', 'FATCA_TRUE']\n",
      "1452 44 0.03\n",
      "363 14 0.04\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data))\n",
    "print(len(train_labels), sum(train_labels), round(sum(train_labels)/len(train_labels), 2))\n",
    "print(len(test_labels), sum(test_labels), round(sum(test_labels)/len(test_labels), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.array(train_data)\n",
    "test_features = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_GBM = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.006 N estimators: 13000 AUC train: 0.902 AUC test: 0.89 Overfit: 0.0126\n",
      "Learning rate: 0.006 N estimators: 14000 AUC train: 0.905 AUC test: 0.89 Overfit: 0.0151\n",
      "Learning rate: 0.006 N estimators: 15000 AUC train: 0.907 AUC test: 0.891 Overfit: 0.0168\n",
      "Learning rate: 0.006 N estimators: 16000 AUC train: 0.909 AUC test: 0.891 Overfit: 0.0187\n",
      "Learning rate: 0.006 N estimators: 17000 AUC train: 0.911 AUC test: 0.891 Overfit: 0.02\n",
      "Learning rate: 0.006 N estimators: 18000 AUC train: 0.913 AUC test: 0.891 Overfit: 0.0211\n",
      "Learning rate: 0.006 N estimators: 19000 AUC train: 0.914 AUC test: 0.892 Overfit: 0.0213\n",
      "Learning rate: 0.006 N estimators: 20000 AUC train: 0.915 AUC test: 0.892 Overfit: 0.0226\n",
      "Learning rate: 0.006 N estimators: 21000 AUC train: 0.916 AUC test: 0.893 Overfit: 0.023\n",
      "Learning rate: 0.007 N estimators: 13000 AUC train: 0.908 AUC test: 0.89 Overfit: 0.0172\n",
      "Learning rate: 0.007 N estimators: 14000 AUC train: 0.91 AUC test: 0.891 Overfit: 0.0189\n",
      "Learning rate: 0.007 N estimators: 15000 AUC train: 0.912 AUC test: 0.891 Overfit: 0.0205\n",
      "Learning rate: 0.007 N estimators: 16000 AUC train: 0.913 AUC test: 0.892 Overfit: 0.0211\n",
      "Learning rate: 0.007 N estimators: 17000 AUC train: 0.915 AUC test: 0.892 Overfit: 0.0228\n",
      "Learning rate: 0.007 N estimators: 18000 AUC train: 0.916 AUC test: 0.893 Overfit: 0.023\n",
      "Learning rate: 0.007 N estimators: 19000 AUC train: 0.918 AUC test: 0.893 Overfit: 0.0243\n",
      "Learning rate: 0.007 N estimators: 20000 AUC train: 0.919 AUC test: 0.892 Overfit: 0.0263\n",
      "Learning rate: 0.007 N estimators: 21000 AUC train: 0.919 AUC test: 0.89 Overfit: 0.0291\n",
      "Learning rate: 0.008 N estimators: 13000 AUC train: 0.912 AUC test: 0.891 Overfit: 0.0208\n",
      "Learning rate: 0.008 N estimators: 14000 AUC train: 0.913 AUC test: 0.892 Overfit: 0.0211\n",
      "Learning rate: 0.008 N estimators: 15000 AUC train: 0.915 AUC test: 0.893 Overfit: 0.0223\n",
      "Learning rate: 0.008 N estimators: 16000 AUC train: 0.917 AUC test: 0.894 Overfit: 0.0227\n",
      "Learning rate: 0.008 N estimators: 17000 AUC train: 0.918 AUC test: 0.893 Overfit: 0.0253\n",
      "Learning rate: 0.008 N estimators: 18000 AUC train: 0.919 AUC test: 0.891 Overfit: 0.0285\n",
      "Learning rate: 0.008 N estimators: 19000 AUC train: 0.92 AUC test: 0.89 Overfit: 0.0307\n",
      "Learning rate: 0.008 N estimators: 20000 AUC train: 0.922 AUC test: 0.889 Overfit: 0.0324\n",
      "Learning rate: 0.008 N estimators: 21000 AUC train: 0.923 AUC test: 0.889 Overfit: 0.0345\n",
      "Learning rate: 0.009 N estimators: 13000 AUC train: 0.915 AUC test: 0.892 Overfit: 0.0223\n",
      "Learning rate: 0.009 N estimators: 14000 AUC train: 0.916 AUC test: 0.893 Overfit: 0.0231\n",
      "Learning rate: 0.009 N estimators: 15000 AUC train: 0.918 AUC test: 0.893 Overfit: 0.0248\n",
      "Learning rate: 0.009 N estimators: 16000 AUC train: 0.919 AUC test: 0.891 Overfit: 0.0285\n",
      "Learning rate: 0.009 N estimators: 17000 AUC train: 0.921 AUC test: 0.89 Overfit: 0.0309\n",
      "Learning rate: 0.009 N estimators: 18000 AUC train: 0.922 AUC test: 0.889 Overfit: 0.033\n",
      "Learning rate: 0.009 N estimators: 19000 AUC train: 0.923 AUC test: 0.888 Overfit: 0.0358\n",
      "Learning rate: 0.009 N estimators: 20000 AUC train: 0.924 AUC test: 0.887 Overfit: 0.0376\n",
      "Learning rate: 0.009 N estimators: 21000 AUC train: 0.925 AUC test: 0.885 Overfit: 0.0398\n",
      "Learning rate: 0.01 N estimators: 13000 AUC train: 0.917 AUC test: 0.892 Overfit: 0.0248\n",
      "Learning rate: 0.01 N estimators: 14000 AUC train: 0.919 AUC test: 0.892 Overfit: 0.0263\n",
      "Learning rate: 0.01 N estimators: 15000 AUC train: 0.92 AUC test: 0.89 Overfit: 0.0305\n",
      "Learning rate: 0.01 N estimators: 16000 AUC train: 0.922 AUC test: 0.889 Overfit: 0.0324\n",
      "Learning rate: 0.01 N estimators: 17000 AUC train: 0.923 AUC test: 0.888 Overfit: 0.0352\n",
      "Learning rate: 0.01 N estimators: 18000 AUC train: 0.924 AUC test: 0.887 Overfit: 0.0376\n",
      "Learning rate: 0.01 N estimators: 19000 AUC train: 0.925 AUC test: 0.885 Overfit: 0.04\n",
      "Learning rate: 0.01 N estimators: 20000 AUC train: 0.926 AUC test: 0.884 Overfit: 0.0427\n",
      "Learning rate: 0.01 N estimators: 21000 AUC train: 0.927 AUC test: 0.882 Overfit: 0.045\n",
      "Learning rate: 0.011 N estimators: 13000 AUC train: 0.919 AUC test: 0.891 Overfit: 0.0281\n",
      "Learning rate: 0.011 N estimators: 14000 AUC train: 0.921 AUC test: 0.889 Overfit: 0.0315\n",
      "Learning rate: 0.011 N estimators: 15000 AUC train: 0.923 AUC test: 0.889 Overfit: 0.0338\n",
      "Learning rate: 0.011 N estimators: 16000 AUC train: 0.924 AUC test: 0.887 Overfit: 0.0368\n",
      "Learning rate: 0.011 N estimators: 17000 AUC train: 0.925 AUC test: 0.886 Overfit: 0.0388\n",
      "Learning rate: 0.011 N estimators: 18000 AUC train: 0.926 AUC test: 0.884 Overfit: 0.0426\n",
      "Learning rate: 0.011 N estimators: 19000 AUC train: 0.927 AUC test: 0.882 Overfit: 0.0447\n",
      "Learning rate: 0.011 N estimators: 20000 AUC train: 0.928 AUC test: 0.881 Overfit: 0.0469\n",
      "Learning rate: 0.011 N estimators: 21000 AUC train: 0.928 AUC test: 0.879 Overfit: 0.0495\n",
      "Learning rate: 0.012 N estimators: 13000 AUC train: 0.921 AUC test: 0.889 Overfit: 0.0318\n",
      "Learning rate: 0.012 N estimators: 14000 AUC train: 0.923 AUC test: 0.889 Overfit: 0.0345\n",
      "Learning rate: 0.012 N estimators: 15000 AUC train: 0.924 AUC test: 0.887 Overfit: 0.0376\n",
      "Learning rate: 0.012 N estimators: 16000 AUC train: 0.926 AUC test: 0.885 Overfit: 0.0404\n",
      "Learning rate: 0.012 N estimators: 17000 AUC train: 0.927 AUC test: 0.883 Overfit: 0.0439\n",
      "Learning rate: 0.012 N estimators: 18000 AUC train: 0.927 AUC test: 0.882 Overfit: 0.0455\n",
      "Learning rate: 0.012 N estimators: 19000 AUC train: 0.928 AUC test: 0.88 Overfit: 0.0485\n",
      "Learning rate: 0.012 N estimators: 20000 AUC train: 0.929 AUC test: 0.877 Overfit: 0.0518\n",
      "Learning rate: 0.012 N estimators: 21000 AUC train: 0.93 AUC test: 0.876 Overfit: 0.0545\n"
     ]
    }
   ],
   "source": [
    "for lr in range(60, 125, 10):\n",
    "    for ne in range(13000, 22000, 1000):\n",
    "        model = lgb.LGBMClassifier(max_depth=1, learning_rate=lr/100000, n_estimators=ne, is_unbalance=True)#,is_unbalance=True\n",
    "        model.fit(train_data, train_labels)\n",
    "        test_predictions = model.predict_proba(test_data, raw_score=True)\n",
    "        train_predictions = model.predict_proba(train_data, raw_score=True)\n",
    "        auc_test = roc_auc_score(test_labels, test_predictions)\n",
    "        auc_train = roc_auc_score(train_labels, train_predictions)\n",
    "        overfit = round(auc_train - auc_test, 4)\n",
    "        results_GBM.append({'max_depth': 1,\n",
    "                            'learning_rate': lr/100000,\n",
    "                            'n_estimators': ne, \n",
    "                            'auc_train': auc_train,\n",
    "                            'auc_test': auc_test,\n",
    "                            'overfit': overfit})\n",
    "        print('Learning rate:', lr/10000, 'N estimators:', ne, 'AUC train:',\n",
    "              round(auc_train, 3), 'AUC test:', round(auc_test, 3), 'Overfit:', overfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic model - baseline for auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC train: 0.948 AUC test: 0.849 Overfit: 0.0995\n"
     ]
    }
   ],
   "source": [
    "# save default parameters\n",
    "# model = lgb.LGBMClassifier(max_depth=1, learning_rate=0.001 , n_estimators=15000)#,is_unbalance=True\n",
    "# model = lgb.LGBMClassifier(max_depth=1, learning_rate=0.0006 , n_estimators=13200)#,is_unbalance=True\n",
    "\n",
    "model = lgb.LGBMClassifier(max_depth=1, learning_rate=0.006, n_estimators=13000) #,is_unbalance=True\n",
    "model.fit(train_data, train_labels)\n",
    "# train_time = timer() - start\n",
    "test_predictions = model.predict_proba(test_data, raw_score=True)\n",
    "train_predictions = model.predict_proba(train_data, raw_score=True)\n",
    "auc_test = roc_auc_score(test_labels, test_predictions)\n",
    "auc_train = roc_auc_score(train_labels, train_predictions)\n",
    "overfit = round(auc_train - auc_test, 4)\n",
    "print('AUC train:', round(auc_train, 3), 'AUC test:', round(auc_test, 3), 'Overfit:', overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_GBM)\n",
    "results_df.to_excel(r'model_training\\results\\PC\\gbm\\gbm_cat_all_boruta.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = pd.DataFrame(test_predictions)\n",
    "train_predictions_df = pd.DataFrame(train_predictions)\n",
    "\n",
    "test_results = pd.concat([test_id, test_datestamp, test_predictions_df, test_labels], axis=1)\n",
    "test_results = test_results.rename(columns={0: \"Score\"})\n",
    "test_results = test_results.sort_values(by='Score', axis=0, ascending=False)\n",
    "\n",
    "SumTest = pd.DataFrame(np.cumsum(test_results['Label']))\n",
    "SumTest = SumTest.rename(columns={'Label': 'Sum'})\n",
    "test_results = pd.concat([test_results, SumTest], axis=1)\n",
    "test_results.reset_index(inplace = True , drop=True)\n",
    "X_coordinates = pd.DataFrame(np.arange(1,(test_results.shape[0]+1))/(test_results.shape[0]))\n",
    "X_coordinates = X_coordinates.rename(columns={0: 'X_coordinates'})\n",
    "y_coordinates = pd.DataFrame(test_results['Sum']/test_results['Sum'].max())\n",
    "y_coordinates = y_coordinates.rename(columns={'Sum': 'y_coordinates'})\n",
    "test_results = pd.concat([test_results, X_coordinates,y_coordinates], axis=1)\n",
    "\n",
    "# train results\n",
    "train_results = pd.concat([train_id, train_datestamp, train_predictions_df, train_labels], axis=1)\n",
    "train_results = train_results.rename(columns={0:\"Score\"})\n",
    "train_results = train_results.sort_values(by='Score', axis=0, ascending=False)\n",
    "\n",
    "SumTrain = pd.DataFrame(np.cumsum(train_results['Label']))\n",
    "SumTrain = SumTrain.rename(columns={'Label':'Sum'})\n",
    "train_results = pd.concat([train_results, SumTrain], axis=1)\n",
    "\n",
    "train_results.reset_index(inplace = True , drop=True)\n",
    "X_coordinates = pd.DataFrame(np.arange(1,(train_results.shape[0]+1))/(train_results.shape[0]))\n",
    "X_coordinates = X_coordinates.rename(columns={0: 'X_coordinates'})\n",
    "y_coordinates = pd.DataFrame(train_results['Sum']/train_results['Sum'].max())\n",
    "y_coordinates = y_coordinates.rename(columns={'Sum': 'y_coordinates'})\n",
    "train_results = pd.concat([train_results, X_coordinates,y_coordinates], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PCScreening_Major_last_5_checklists', 'PCRelationship_Major_last_5_checklists', 'DRDocumentation_standard_Minor_last_10_checklists', 'TLAssignedName_Skrzynecki__P___Piotr_', 'DRClient_Outreach_Critical_last_10_checklists', 'DRClient_Outreach_Major_last_5_checklists', 'DRClient_Outreach_Major_last_10_checklists', 'ProjectExperience', 'PCDocumentation_standard_Minor_last_5_checklists', 'PCScreening_Major_last_10_checklists', 'TeamExperience', 'PCDocumentation_standard_Major_last_10_checklists', 'PCDocumentation_standard_Minor_last_10_checklists', 'PCDocumentation_standard_Major_last_5_checklists']\n"
     ]
    }
   ],
   "source": [
    "test_results.to_csv(r'model_training/results/PC/gbm/gbm_test_results.csv')\n",
    "train_results.to_csv(r'model_training/results/PC/gbm/gbm_train_results.csv')\n",
    "model.booster_.save_model(r'model_training/results/PC/gbm/gbm_best_model.txt')\n",
    "#model = lgb.Booster(model_file=r'gbm_best_model.txt')\n",
    "print(list(train_data.columns))\n",
    "with open('model_training/results/PC/gbm/used_features.txt', 'w') as f:\n",
    "    for item in train_data.columns:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian hyperoptimalization - has a tendency to overfit, therefore watch out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N_FOLDS=3\n",
    "\n",
    "\n",
    "def objective(params, n_folds = N_FOLDS):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Retrieve the subsample if present otherwise set to 1.0\n",
    "    subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Extract the boosting type\n",
    "    params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "    params['subsample'] = subsample\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(params, lgb.Dataset(train_data,train_labels), num_boost_round = 10000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    best_score = np.max(cv_results['auc-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "            'estimators': n_estimators, \n",
    "            'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the learning rate\n",
    "learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.5))}\n",
    "num_leaves = {'num_leaves': hp.quniform('num_leaves', 20, 250, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_type = {'boosting_type': hp.choice('boosting_type', \n",
    "                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n",
    "                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n",
    "# Draw a sample\n",
    "params = sample(boosting_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "\n",
    "# Extract the boosting type\n",
    "params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "params['subsample'] = subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'max_depth' : hp.choice('max_depth',np.arange(1, 14, dtype=int)),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe_algorithm = tpe.suggest\n",
    "bayes_trials = Trials()\n",
    "\n",
    "out_file = 'gbm_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Global variable\n",
    "global  ITERATION\n",
    "\n",
    "ITERATION = 10\n",
    "MAX_EVALS=50\n",
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'goss',\n",
       " 'class_weight': 'balanced',\n",
       " 'colsample_bytree': 0.7292016631698011,\n",
       " 'learning_rate': 0.010705097915397449,\n",
       " 'min_child_samples': 20,\n",
       " 'num_leaves': 39,\n",
       " 'reg_alpha': 0.6689164372874457,\n",
       " 'reg_lambda': 0.5471632266906857,\n",
       " 'subsample_for_bin': 100000,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the trials with lowest loss (highest AUC) first\n",
    "#bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "results = pd.read_csv('gbm_trials.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()\n",
    "\n",
    "\n",
    "# Convert from a string to a dictionary\n",
    "ast.literal_eval(results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7069978597622248"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', random_state = 50, **best_bayes_params,max_depth=5)\n",
    "\n",
    "best_bayes_model.fit(train_data, train_labels)\n",
    "\n",
    "test_features=np.array(test_data)\n",
    "train_features=np.array(train_data)\n",
    "test_predictions = best_bayes_model.predict_proba(test_features)[:, 1]\n",
    "train_predictions=best_bayes_model.predict_proba(train_features)[:, 1]\n",
    "\n",
    "\n",
    "#auc_best_model = roc_auc_score(test_labels, test_predictions)\n",
    "auc_best_model = roc_auc_score(test_labels, test_predictions)\n",
    "auc_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'goss',\n",
       " 'class_weight': 'balanced',\n",
       " 'colsample_bytree': 0.7292016631698011,\n",
       " 'learning_rate': 0.010705097915397449,\n",
       " 'min_child_samples': 20,\n",
       " 'num_leaves': 39,\n",
       " 'reg_alpha': 0.6689164372874457,\n",
       " 'reg_lambda': 0.5471632266906857,\n",
       " 'subsample_for_bin': 100000,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_bayes_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7967594193300186"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_best_model_train = roc_auc_score(train_labels, train_predictions)\n",
    "auc_best_model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict_proba(test_features)[:, 1]\n",
    "train_predictions=model.predict_proba(train_features)[:, 1]\n",
    "\n",
    "test_predictions_df = pd.DataFrame(test_predictions)\n",
    "train_predictions_df = pd.DataFrame(train_predictions)\n",
    "\n",
    "test_results = pd.concat([test_id, test_datestamp test_predictions_df, test_labels], axis=1)\n",
    "test_results = test_results.rename(columns={0: \"Score\"})\n",
    "test_results = test_results.sort_values(by='Score', axis=0, ascending=False)\n",
    "\n",
    "SumTest = pd.DataFrame(np.cumsum(test_results['Label']))\n",
    "SumTest = SumTest.rename(columns={'Label': 'Sum'})\n",
    "test_results = pd.concat([test_results, SumTest], axis=1)\n",
    "test_results.reset_index(inplace = True , drop=True)\n",
    "X_coordinates = pd.DataFrame(np.arange(1,(test_results.shape[0]+1))/(test_results.shape[0]))\n",
    "X_coordinates = X_coordinates.rename(columns={0: 'X_coordinates'})\n",
    "y_coordinates = pd.DataFrame(test_results['Sum']/test_results['Sum'].max())\n",
    "y_coordinates = y_coordinates.rename(columns={'Sum': 'y_coordinates'})\n",
    "test_results = pd.concat([test_results, X_coordinates,y_coordinates], axis=1)\n",
    "\n",
    "# train results\n",
    "train_results = pd.concat([train_id, train_datestamp, train_predictions_df, train_labels], axis=1)\n",
    "train_results = train_results.rename(columns={0:\"Score\"})\n",
    "train_results = train_results.sort_values(by='Score', axis=0, ascending=False)\n",
    "\n",
    "SumTrain = pd.DataFrame(np.cumsum(train_results['Label']))\n",
    "SumTrain = SumTrain.rename(columns={'Label':'Sum'})\n",
    "train_results = pd.concat([train_results, SumTrain], axis=1)\n",
    "\n",
    "train_results.reset_index(inplace = True , drop=True)\n",
    "X_coordinates = pd.DataFrame(np.arange(1,(train_results.shape[0]+1))/(train_results.shape[0]))\n",
    "X_coordinates = X_coordinates.rename(columns={0: 'X_coordinates'})\n",
    "y_coordinates = pd.DataFrame(train_results['Sum']/train_results['Sum'].max())\n",
    "y_coordinates = y_coordinates.rename(columns={'Sum': 'y_coordinates'})\n",
    "train_results = pd.concat([train_results, X_coordinates,y_coordinates], axis=1)\n",
    "\n",
    "test_results.to_csv(r'gbm_test_results.csv')\n",
    "train_results.to_csv(r'gbm_train_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1ebbc295c08>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.booster_.save_model(r'gbm_best_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = lgb.Booster(model_file=r'C:/Users/kzielinski003/Desktop/Chocolate/gbm_best_model_major.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
