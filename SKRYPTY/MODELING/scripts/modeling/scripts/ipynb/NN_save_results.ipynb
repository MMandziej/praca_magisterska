{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from keras import losses, optimizers\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from statistics import mean\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics, losses\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.activations import elu, exponential, hard_sigmoid, linear, \\\n",
    "    relu, sigmoid, softmax, tanh\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, \\\n",
    "    logcosh, mean_squared_error, poisson, mean_absolute_error\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam, Nadam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\mmandziej001\\Desktop\\FCU\\SCRIPTS\\predictive_qc_db')\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(r'models\\results\\nn\\cat4\\seconditeration\\nn_results_boruta_2020-05-19.h5')\n",
    "\n",
    "train_data = pd.read_csv(r'models/data/cat4/dataset_dummy_grouped_time_train_cat4_inc.csv')\n",
    "test_data = pd.read_csv(r'models/data/cat4/dataset_dummy_grouped_time_test_cat4_inc.csv')\n",
    "\n",
    "boruta_importance = pd.read_excel(r'data/ai_assistant_dumps/cat4/Boruta_variable_importance2.xlsx')\n",
    "boruta_importance = boruta_importance.sort_values(by=['normHits', 'meanImp'],\n",
    "                                                  ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), sum(train_data['Label']), round(sum(train_data['Label'])/len(train_data), 3))\n",
    "print(len(test_data), sum(test_data['Label']), round(sum(test_data['Label'])/len(test_data), 3))\n",
    "\n",
    "\"\"\"\n",
    "full_dataset = pd.concat([train_data, test_data])\n",
    "train_data, test_data = train_test_split(full_dataset,\n",
    "                                         test_size=0.2,\n",
    "                                         shuffle=True,\n",
    "                                         random_state=100)\n",
    "\n",
    "print('New approach: ')\n",
    "print(len(train_data), sum(train_data['Label']), round(sum(train_data['Label'])/len(train_data), 3))\n",
    "print(len(test_data), sum(test_data['Label']), round(sum(test_data['Label'])/len(test_data), 3))\n",
    "\n",
    "train_data.to_csv(r\"models\\results\\nn\\cat6\\newapproach\\train_data.csv\")\n",
    "test_data.to_csv(r\"models\\results\\nn\\cat6\\newapproach\\test_data.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586 321 0.202\n",
      "339 52 0.153\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), sum(train_data['Label']), round(sum(train_data['Label'])/len(train_data), 3))\n",
    "print(len(test_data), sum(test_data['Label']), round(sum(test_data['Label'])/len(test_data), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unique',\n",
       " 'analyst_cases_processed_30days',\n",
       " 'analyst_cases_processed_90days',\n",
       " 'analyst_cases_processed_all',\n",
       " 'analyst_cases_average_score_all',\n",
       " 'analyst_expierence_day',\n",
       " 'analyst_score_deducted_1_all',\n",
       " 'analyst_score_deducted_2_all',\n",
       " 'analyst_score_deducted_4_all',\n",
       " 'analyst_score_deducted_5_all',\n",
       " 'analyst_score_deducted_6_all',\n",
       " 'analyst_score_deducted_7_all',\n",
       " 'analyst_score_deducted_8_all',\n",
       " 'analyst_score_deducted_9_all',\n",
       " 'analyst_score_deducted_10_all',\n",
       " 'analyst_score_deducted_11_all',\n",
       " 'analyst_score_deducted_12_all',\n",
       " 'analyst_score_deducted_13_all',\n",
       " 'analyst_score_deducted_14_all',\n",
       " 'analyst_score_deducted_15_all',\n",
       " 'analyst_score_deducted_17_all',\n",
       " 'analyst_score_deducted_18_all',\n",
       " 'analyst_error_binary_1_all',\n",
       " 'analyst_error_binary_2_all',\n",
       " 'analyst_error_binary_4_all',\n",
       " 'analyst_error_binary_5_all',\n",
       " 'analyst_error_binary_6_all',\n",
       " 'analyst_error_binary_7_all',\n",
       " 'analyst_error_binary_8_all',\n",
       " 'analyst_error_binary_9_all',\n",
       " 'analyst_error_binary_10_all',\n",
       " 'analyst_error_binary_11_all',\n",
       " 'analyst_error_binary_12_all',\n",
       " 'analyst_error_binary_13_all',\n",
       " 'analyst_error_binary_14_all',\n",
       " 'analyst_error_binary_15_all',\n",
       " 'analyst_error_binary_17_all',\n",
       " 'analyst_error_binary_18_all',\n",
       " 'DDLevel_V1',\n",
       " 'DDLevel_CDD',\n",
       " 'DDLevel_EDD',\n",
       " 'DDLevel_SDD',\n",
       " 'Entity_Collective_Investment_Scheme',\n",
       " 'Entity_DB_Recognised_Exchange_Listed_Entity',\n",
       " 'Entity_DB_Recognised_Regulated_Entity',\n",
       " 'Entity_Fintech',\n",
       " 'Entity_Government_entity',\n",
       " 'Entity_Other_entity',\n",
       " 'Entity_Parent_Exchange',\n",
       " 'Entity_Private_Entity',\n",
       " 'Entity_SPV',\n",
       " 'Kop_V1',\n",
       " 'Kop_Apollo_v3_2',\n",
       " 'Kop_Apollo_v4_0',\n",
       " 'Kop_v3_2',\n",
       " 'Kop_v4_0',\n",
       " 'Kop_v4_1',\n",
       " 'High_Value_Client_1_E',\n",
       " 'High_Value_Client_1_F',\n",
       " 'High_Value_Client_1_NM',\n",
       " 'High_Value_Client_1_Z',\n",
       " 'High_Value_Client_2_F',\n",
       " 'High_Value_Client_3_F',\n",
       " 'High_Value_Client_4_B',\n",
       " 'High_Value_Client_4_F',\n",
       " 'High_Value_Client_4_Z',\n",
       " 'High_Value_Client_5_C',\n",
       " 'High_Value_Client_5_E',\n",
       " 'High_Value_Client_5_F',\n",
       " 'High_Value_Client_5_Z',\n",
       " 'High_Value_Client_6_D',\n",
       " 'High_Value_Client_6_F',\n",
       " 'High_Value_Client_7_E',\n",
       " 'High_Value_Client_7_F',\n",
       " 'High_Value_Client_7_Z',\n",
       " 'High_Value_Client_8_NM',\n",
       " 'High_Value_Client_Bronze',\n",
       " 'High_Value_Client_NULL',\n",
       " 'High_Value_Client_Other_entity',\n",
       " 'High_Value_Client_Platinum',\n",
       " 'High_Value_Client_Silver',\n",
       " 'Label',\n",
       " 'datestamp']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_data)\n",
    "#del train_data['Unnamed: 0']\n",
    "#del test_data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train_data.pop('Unique')\n",
    "test_id = test_data.pop('Unique')\n",
    "\n",
    "train_labels = train_data.pop('Label')\n",
    "test_labels = test_data.pop('Label')\n",
    "\n",
    "train_datestamp = train_data.pop('datestamp')\n",
    "test_datestamp = test_data.pop('datestamp')\n",
    "\n",
    "qc_columns = [col for col in train_data.columns if 'qc_' in col]\n",
    "used_columns = [col for col in train_data.columns if col not in qc_columns]\n",
    "boruta_features = list(\n",
    "    boruta_importance[boruta_importance['decision'] == 'Confirmed'].V7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6933766761479073 0.7003819351380328\n"
     ]
    }
   ],
   "source": [
    "test_data = test_data[boruta_features] # used_columns, boruta_features, models_cols\n",
    "train_data = train_data[boruta_features] # used_columns, boruta_features, models_cols\n",
    "\n",
    "test_predictions = best_model.predict(test_data)\n",
    "train_predictions = best_model.predict(train_data)\n",
    "\n",
    "test_predictions_df = pd.DataFrame(test_predictions)\n",
    "train_predictions_df = pd.DataFrame(train_predictions)\n",
    "\n",
    "# test results\n",
    "test_results = pd.concat([test_id, test_datestamp, test_predictions_df, test_labels], axis=1)\n",
    "test_results = test_results.rename(columns={0: \"Score\"})\n",
    "test_results = test_results.sort_values(by='Score', axis=0, ascending=False)\n",
    "\n",
    "SumTest = pd.DataFrame(np.cumsum(test_results['Label']))\n",
    "SumTest = SumTest.rename(columns={'Label': 'Sum'})\n",
    "test_results = pd.concat([test_results, SumTest], axis=1)\n",
    "test_results.reset_index(inplace=True, drop=True)\n",
    "X_coordinates = pd.DataFrame(np.arange(1,(test_results.shape[0]+1))/(test_results.shape[0]))\n",
    "X_coordinates = X_coordinates.rename(columns={0: 'X_coordinates'})\n",
    "y_coordinates = pd.DataFrame(test_results['Sum']/test_results['Sum'].max())\n",
    "y_coordinates = y_coordinates.rename(columns={'Sum': 'y_coordinates'})\n",
    "test_results = pd.concat([test_results, X_coordinates,y_coordinates], axis=1)\n",
    "\n",
    "# train results\n",
    "train_results = pd.concat([train_id, train_datestamp, train_predictions_df, train_labels], axis=1)\n",
    "train_results = train_results.rename(columns={0:\"Score\"})\n",
    "train_results = train_results.sort_values(by='Score', axis=0, ascending=False)\n",
    "\n",
    "SumTrain = pd.DataFrame(np.cumsum(train_results['Label']))\n",
    "SumTrain = SumTrain.rename(columns={'Label':'Sum'})\n",
    "train_results = pd.concat([train_results, SumTrain], axis=1)\n",
    "\n",
    "train_results.reset_index(inplace=True , drop=True)\n",
    "X_coordinates = pd.DataFrame(np.arange(1,(train_results.shape[0]+1))/(train_results.shape[0]))\n",
    "X_coordinates = X_coordinates.rename(columns={0: 'X_coordinates'})\n",
    "y_coordinates = pd.DataFrame(train_results['Sum']/train_results['Sum'].max())\n",
    "y_coordinates = y_coordinates.rename(columns={'Sum': 'y_coordinates'})\n",
    "train_results = pd.concat([train_results, X_coordinates,y_coordinates], axis=1)\n",
    "\n",
    "auc_test = roc_auc_score(test_labels, test_predictions_df)\n",
    "auc_train = roc_auc_score(train_labels, train_predictions_df)\n",
    "print(auc_train, auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.to_csv(r'models/results/nn/cat4/nn_test_results.csv')\n",
    "train_results.to_csv(r'models/results/nn/cat4/nn_train_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
